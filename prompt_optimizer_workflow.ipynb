{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompt Optimizer Workflow\n",
        "\n",
        "This notebook implements a multi-agent system for generating and optimizing prompts using Google Gemini models.\n",
        "\n",
        "## Workflow:\n",
        "1. User enters an idea for a prompt\n",
        "2. Question Generator Agent asks 3-5 clarification questions\n",
        "3. User answers the questions\n",
        "4. Prompt Generator Agent creates the prompt (using gemini-2.5-flash)\n",
        "5. Prompt Tester Agent tests the prompt (using gemini-2.5-pro)\n",
        "6. Prompt Analyzer Agent scores the prompt and output (0-100)\n",
        "7. If score < 100, loop back to improve (max 15 iterations)\n",
        "8. Display final prompt to user\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-generativeai in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (0.8.5)\n",
            "Requirement already satisfied: streamlit in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (1.51.0)\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: pydantic in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-generativeai) (2.12.4)\n",
            "Requirement already satisfied: tqdm in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (2.3.4)\n",
            "Requirement already satisfied: packaging<26,>=20 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (2.3.3)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (12.0.0)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (21.0.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from streamlit) (6.5.2)\n",
            "Requirement already satisfied: jinja2 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.11.0)\n",
            "Requirement already satisfied: colorama in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: comm>=0.1.3 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipywidgets) (9.7.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
            "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: decorator>=4.3.2 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.18.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1.5 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.11.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data>=0.6.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: wcwidth in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.28.0)\n",
            "Requirement already satisfied: six>=1.5 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\cursor projects\\promptgen\\.venv\\lib\\site-packages (from pydantic->google-generativeai) (0.4.2)\n",
            "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
            "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
            "   ---------------------------------------- 0.0/914.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 914.9/914.9 kB 12.3 MB/s  0:00:00\n",
            "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.2/2.2 MB 14.8 MB/s  0:00:00\n",
            "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
            "\n",
            "   -------------------------- ------------- 2/3 [ipywidgets]\n",
            "   -------------------------- ------------- 2/3 [ipywidgets]\n",
            "   -------------------------- ------------- 2/3 [ipywidgets]\n",
            "   -------------------------- ------------- 2/3 [ipywidgets]\n",
            "   -------------------------- ------------- 2/3 [ipywidgets]\n",
            "   -------------------------- ------------- 2/3 [ipywidgets]\n",
            "   -------------------------- ------------- 2/3 [ipywidgets]\n",
            "   -------------------------- ------------- 2/3 [ipywidgets]\n",
            "   ---------------------------------------- 3/3 [ipywidgets]\n",
            "\n",
            "Successfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Uncomment and run this cell to install required packages\n",
        "%pip install google-generativeai streamlit ipywidgets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: API key not found. Please set it in:\n",
            "  1. Streamlit secrets (st.secrets['GOOGLE_API_KEY'])\n",
            "  2. Environment variable (GOOGLE_API_KEY)\n",
            "  3. Or set GOOGLE_API_KEY directly in this cell\n"
          ]
        }
      ],
      "source": [
        "# Configure your Google API key\n",
        "# Priority: Streamlit secrets > Environment variable > Direct assignment\n",
        "\n",
        "def get_api_key():\n",
        "    \"\"\"Get API key from Streamlit secrets, environment variable, or return None\"\"\"\n",
        "    try:\n",
        "        # Try to import streamlit (only works in Streamlit environment)\n",
        "        import streamlit as st\n",
        "        if hasattr(st, 'secrets') and 'GOOGLE_API_KEY' in st.secrets:\n",
        "            return st.secrets['GOOGLE_API_KEY']\n",
        "    except ImportError:\n",
        "        # Not in Streamlit environment, continue to other methods\n",
        "        pass\n",
        "    except Exception:\n",
        "        # Streamlit not available or other error\n",
        "        pass\n",
        "    \n",
        "    # Fallback to environment variable\n",
        "    return os.getenv('GOOGLE_API_KEY', None)\n",
        "\n",
        "GOOGLE_API_KEY = get_api_key()\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    print(\"Warning: API key not found. Please set it in:\")\n",
        "    print(\"  1. Streamlit secrets (st.secrets['GOOGLE_API_KEY'])\")\n",
        "    print(\"  2. Environment variable (GOOGLE_API_KEY)\")\n",
        "    print(\"  3. Or set GOOGLE_API_KEY directly in this cell\")\n",
        "    GOOGLE_API_KEY = 'AIzaSyCel1FQkF5CHTr3ZDELAb9LH3KmgOIZyuk'  # Fallback for notebook testing\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Model configurations\n",
        "# Note: Model names may need to be adjusted based on available models in the API\n",
        "# Common alternatives: 'gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-2.0-flash-exp'\n",
        "PROMPT_GENERATOR_MODEL = 'gemini-2.5-flash'  # Using flash for prompt generation (user specified: gemini-2.5-flash)\n",
        "PROMPT_TESTER_MODEL = 'gemini-2.5-pro'  # Using pro for testing (user specified: gemini-2.5-pro)\n",
        "ANALYZER_MODEL = 'gemini-2.5-pro'  # Using flash for analysis\n",
        "\n",
        "# Workflow settings\n",
        "MAX_ITERATIONS = 15\n",
        "TARGET_SCORE = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Classes for State Management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class WorkflowState:\n",
        "    \"\"\"Tracks the state of the workflow\"\"\"\n",
        "    user_idea: str = \"\"\n",
        "    clarification_questions: List[str] = None\n",
        "    user_answers: Dict[str, str] = None\n",
        "    current_prompt: str = \"\"\n",
        "    prompt_output: str = \"\"\n",
        "    current_score: int = 0\n",
        "    feedback: str = \"\"\n",
        "    iteration: int = 0\n",
        "    best_prompt: str = \"\"\n",
        "    best_score: int = 0\n",
        "    best_output: str = \"\"\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        if self.clarification_questions is None:\n",
        "            self.clarification_questions = []\n",
        "        if self.user_answers is None:\n",
        "            self.user_answers = {}\n",
        "    \n",
        "    def update_best(self):\n",
        "        \"\"\"Update best prompt if current score is higher\"\"\"\n",
        "        if self.current_score > self.best_score:\n",
        "            self.best_score = self.current_score\n",
        "            self.best_prompt = self.current_prompt\n",
        "            self.best_output = self.prompt_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Question Generator Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QuestionGeneratorAgent:\n",
        "    \"\"\"Generates 3-5 clarification questions based on user's prompt idea\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = PROMPT_GENERATOR_MODEL):\n",
        "        self.model = genai.GenerativeModel(model_name)\n",
        "        self.system_prompt = \"\"\"You are a Question Generator AI agent. Your role is to help users define their prompt requirements by asking comprehensive, targeted questions.\n",
        "\n",
        "When a user shares their idea, ask 3-5 relevant questions to gather essential information:\n",
        "\n",
        "1. **Purpose & Goal**: What is the main objective or desired outcome?\n",
        "2. **Target Audience**: Who will be using or reading this content?\n",
        "3. **Tone & Style**: What tone should the output have? (formal, casual, technical, creative, etc.)\n",
        "4. **Context & Constraints**: Are there specific requirements, limitations, or guidelines to follow?\n",
        "5. **Output Format**: What format should the final output be in? (essay, bullet points, code, summary, etc.)\n",
        "6. **Key Elements**: What specific elements or information must be included?\n",
        "7. **Examples**: Are there any examples or references to follow?\n",
        "\n",
        "Ask these questions in a clear, conversational manner. Wait for the user's responses before proceeding. Your output should be ONLY the questions - do not create the prompt yet. Number each question clearly.\"\"\"\n",
        "    \n",
        "    def generate_questions(self, user_idea: str) -> List[str]:\n",
        "        \"\"\"Generate clarification questions based on user's idea\"\"\"\n",
        "        prompt = f\"\"\"{self.system_prompt}\n",
        "\n",
        "User's idea: {user_idea}\n",
        "\n",
        "Please ask 3-5 relevant questions to clarify the requirements.\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            questions_text = response.text.strip()\n",
        "            \n",
        "            # Parse questions (numbered list)\n",
        "            questions = self._parse_questions(questions_text)\n",
        "            return questions\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating questions: {e}\")\n",
        "            # Fallback questions\n",
        "            return [\n",
        "                \"What is the main purpose or goal of this prompt?\",\n",
        "                \"Who is the target audience for the output?\",\n",
        "                \"What tone or style should the output have?\"\n",
        "            ]\n",
        "    \n",
        "    def _parse_questions(self, text: str) -> List[str]:\n",
        "        \"\"\"Parse questions from the response text\"\"\"\n",
        "        # Split by numbered patterns (1., 2., etc.) or question marks\n",
        "        questions = []\n",
        "        # Try to split by numbered list\n",
        "        pattern = r'\\d+[.)]\\s*(.+?)(?=\\d+[.)]|$)'\n",
        "        matches = re.findall(pattern, text, re.DOTALL)\n",
        "        \n",
        "        if matches:\n",
        "            questions = [q.strip() for q in matches]\n",
        "        else:\n",
        "            # Fallback: split by question marks\n",
        "            parts = re.split(r'\\?+', text)\n",
        "            questions = [q.strip() + '?' for q in parts if q.strip() and '?' not in q]\n",
        "        \n",
        "        # Clean up questions\n",
        "        questions = [q for q in questions if len(q) > 10]  # Filter very short strings\n",
        "        \n",
        "        # Ensure we have 3-5 questions\n",
        "        if len(questions) < 3:\n",
        "            questions = [q.strip() for q in text.split('\\n') if '?' in q and len(q.strip()) > 10][:5]\n",
        "        \n",
        "        return questions[:5]  # Max 5 questions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptGeneratorAgent:\n",
        "    \"\"\"Generates optimized prompts based on user idea and clarification answers\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = PROMPT_GENERATOR_MODEL):\n",
        "        self.model = genai.GenerativeModel(\n",
        "            model_name,\n",
        "            generation_config=genai.types.GenerationConfig(temperature=0.1)\n",
        "        )\n",
        "        self.system_prompt = \"\"\"You are an expert prompt engineer. Based on the user's answers to the questions, create a comprehensive and well-structured prompt that addresses all the requirements. The prompt should be clear, specific, and optimized for achieving the user's goals. Include all relevant details about purpose, audience, tone, constraints, and desired output format. Return ONLY the created prompt text without any additional explanation. The prompt must be XML tagged. Below are the tags that you can use:\n",
        "<role> (Mandatory Tag)\n",
        "<context> (Mandatory Tag)\n",
        "<instructions> (Mandatory Tag)\n",
        "<skills> (Mandatory Tag)\n",
        "<areas of knowledge> (Mandatory Tag)\n",
        "<next steps> (Mandatory Tag)\n",
        "<personality and style> (Mandatory Tag)\n",
        "<output format> (Mandatory Tag)\n",
        "<audience> (Optional)\n",
        "<examples> (Optional)\n",
        "<dont's> (Optional)\"\"\"\n",
        "    \n",
        "    def generate_prompt(self, user_idea: str, user_answers: Dict[str, str], \n",
        "                       feedback: str = \"\") -> str:\n",
        "        \"\"\"Generate a prompt based on user idea and answers\"\"\"\n",
        "        \n",
        "        # Format user answers\n",
        "        answers_text = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in user_answers.items()])\n",
        "        \n",
        "        if feedback:\n",
        "            prompt = f\"\"\"{self.system_prompt}\n",
        "\n",
        "Previous prompt feedback: {feedback}\n",
        "\n",
        "User's original idea: {user_idea}\n",
        "\n",
        "User's clarification answers:\n",
        "{answers_text}\n",
        "\n",
        "Based on the feedback, improve the prompt to address the issues mentioned. Return ONLY the improved prompt with XML tags.\"\"\"\n",
        "        else:\n",
        "            prompt = f\"\"\"{self.system_prompt}\n",
        "\n",
        "User's original idea: {user_idea}\n",
        "\n",
        "User's clarification answers:\n",
        "{answers_text}\n",
        "\n",
        "Create a comprehensive prompt based on the above information. Return ONLY the prompt with XML tags.\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating prompt: {e}\")\n",
        "            return \"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptTesterAgent:\n",
        "    \"\"\"Tests the generated prompt by running it on the model\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = PROMPT_TESTER_MODEL):\n",
        "        self.model = genai.GenerativeModel(\n",
        "            model_name,\n",
        "            generation_config=genai.types.GenerationConfig(temperature=0.2)\n",
        "        )\n",
        "    \n",
        "    def test_prompt(self, prompt: str) -> str:\n",
        "        \"\"\"Run the prompt on the model and return the output\"\"\"\n",
        "        try:\n",
        "            # The prompt itself is the input to the model\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text.strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Error testing prompt: {e}\")\n",
        "            return f\"Error: {str(e)}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prompt Analyzer Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptAnalyzerAgent:\n",
        "    \"\"\"Analyzes and scores prompts based on multiple parameters\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = ANALYZER_MODEL):\n",
        "        self.model = genai.GenerativeModel(\n",
        "            model_name,\n",
        "            generation_config=genai.types.GenerationConfig(temperature=0.2)\n",
        "        )\n",
        "        \n",
        "        self.analysis_prompt_template = \"\"\"You will take the prompt [{prompt}] and benchmark it against the following parameters:\n",
        "\n",
        "**Prompt Benchmark Attributes:**\n",
        "\n",
        "1. **Clarity & Unambiguity**: How easily understandable is the prompt's language and intent?\n",
        "   - 1: Very ambiguous or confusing language.\n",
        "   - 3: Mostly clear, but requires some interpretation.\n",
        "   - 5: Perfectly clear, intent is obvious.\n",
        "\n",
        "2. **Specificity & Detail**: How precisely does the prompt define the desired outcome and its characteristics?\n",
        "   - 1: Very vague, lacks necessary details.\n",
        "   - 3: Moderately specific, includes some key details.\n",
        "   - 5: Highly specific, provides comprehensive details about the desired output.\n",
        "\n",
        "3. **Contextual Sufficiency**: Does the prompt provide all necessary background information for the AI to understand the task domain?\n",
        "   - 1: Lacks critical context, making the task difficult to perform accurately.\n",
        "   - 3: Provides some necessary context, but gaps remain.\n",
        "   - 5: Provides all relevant context required for the task.\n",
        "\n",
        "4. **Constraint Definition**: How clearly are limitations (e.g., length, format, tone, exclusions, required elements) stated?\n",
        "   - 1: No constraints defined, or constraints are unclear.\n",
        "   - 3: Some relevant constraints are mentioned but may be incomplete or slightly ambiguous.\n",
        "   - 5: All relevant constraints are clearly and explicitly defined.\n",
        "\n",
        "5. **Role/Persona Clarity**: If a specific role or persona for the AI is requested, how clearly is it defined? (Score NA if no role is needed).\n",
        "   - 1: Role is undefined or very confusing.\n",
        "   - 3: Role is mentioned but lacks detail or nuance.\n",
        "   - 5: Role is clearly defined with sufficient detail to guide the AI's perspective.\n",
        "\n",
        "6. **Task Definition**: How clearly is the primary goal or task articulated? What should the AI do?\n",
        "   - 1: The core task is unclear or poorly defined.\n",
        "   - 3: The task is generally understandable but could be more precise.\n",
        "   - 5: The task is explicitly and accurately defined.\n",
        "\n",
        "7. **Conciseness & Efficiency**: Is the prompt free from irrelevant information, redundancy, or excessive length?\n",
        "   - 1: Very verbose, contains significant irrelevant information.\n",
        "   - 3: Mostly concise, with minor irrelevant parts.\n",
        "   - 5: Optimally concise, containing only necessary information.\n",
        "\n",
        "8. **Structural Organization**: Is the prompt logically structured (e.g., using formatting, clear separation of instructions)?\n",
        "   - 1: Disorganized, hard to follow instructions.\n",
        "   - 3: Moderately organized, structure could be improved.\n",
        "   - 5: Well-structured, logical flow, easy to parse.\n",
        "\n",
        "9. **Instructional Effectiveness**: How likely are the specific instructions and phrasing to lead directly to the desired output, minimizing potential misinterpretation?\n",
        "   - 1: Instructions are likely to be misinterpreted or fail.\n",
        "   - 3: Instructions are somewhat effective but could lead to deviations.\n",
        "   - 5: Instructions are highly effective and precisely guide the AI.\n",
        "\n",
        "10. **Tone/Style Guidance**: How clearly is the desired tone (e.g., formal, friendly, technical) or writing style (e.g., simple, academic, bullet points) specified? (Score NA if not relevant).\n",
        "    - 1: No guidance, or guidance is vague/contradictory.\n",
        "    - 3: Some guidance provided, but could be more specific.\n",
        "    - 5: Clear, specific, and consistent guidance on tone/style.\n",
        "\n",
        "Next you will take the prompt output [{output}] and benchmark it against the following parameters:\n",
        "\n",
        "**Output Benchmark Attributes:**\n",
        "\n",
        "1. **Relevance to Prompt**: How directly does the output address the core request and topic of the prompt?\n",
        "   - 1: Off-topic or completely misses the prompt's intent.\n",
        "   - 3: Partially relevant, addresses some aspects but deviates or includes irrelevant info.\n",
        "   - 5: Directly and fully relevant to the prompt's core request.\n",
        "\n",
        "2. **Factual Accuracy**: How correct is the factual information presented in the output? (Score NA if subjective/creative).\n",
        "   - 1: Contains significant factual errors or hallucinations.\n",
        "   - 3: Mostly accurate, but contains minor inaccuracies.\n",
        "   - 5: Completely factually accurate.\n",
        "\n",
        "3. **Completeness of Response**: Does the output address all explicit and implicit requirements, questions, or parts of the prompt?\n",
        "   - 1: Misses major requirements or questions asked.\n",
        "   - 3: Addresses most requirements but omits minor aspects.\n",
        "   - 5: Fully addresses all requirements specified in the prompt.\n",
        "\n",
        "4. **Coherence & Logical Flow**: Is the output well-organized, with logical connections between ideas/sentences/paragraphs?\n",
        "   - 1: Incoherent, disjointed, very difficult to follow.\n",
        "   - 3: Mostly coherent, but some transitions or points are unclear.\n",
        "   - 5: Very coherent, logical flow, easy to follow.\n",
        "\n",
        "5. **Clarity & Readability**: Is the language used clear, grammatically correct, and easy for the target audience to understand?\n",
        "   - 1: Very difficult to understand, poor grammar, confusing language.\n",
        "   - 3: Mostly clear, but some awkward phrasing or minor grammatical errors.\n",
        "   - 5: Very clear, well-written, and easily readable.\n",
        "\n",
        "6. **Constraint Adherence**: Does the output respect all constraints (length, format, style, exclusions) specified in the prompt?\n",
        "   - 1: Ignores most or all specified constraints.\n",
        "   - 3: Meets some constraints but violates others.\n",
        "   - 5: Meets all specified constraints perfectly.\n",
        "\n",
        "7. **Tone & Style Consistency**: Does the output consistently match the tone and style requested (or implied) by the prompt?\n",
        "   - 1: Uses a completely inappropriate tone/style or is highly inconsistent.\n",
        "   - 3: Mostly matches the requested tone/style but has inconsistencies.\n",
        "   - 5: Perfectly and consistently matches the requested tone/style.\n",
        "\n",
        "8. **Formatting & Presentation**: Is the output presented in a clean, usable, and readable format (using markdown, lists, code blocks, etc., appropriately)?\n",
        "   - 1: Poor or absent formatting, difficult to read/use.\n",
        "   - 3: Adequate formatting, generally readable.\n",
        "   - 5: Excellent formatting and presentation enhances readability and usability.\n",
        "\n",
        "9. **Depth & Elaboration**: Does the output provide sufficient detail, explanation, or depth appropriate for the prompt's request? (Consider if the prompt asked for brevity vs. detail).\n",
        "   - 1: Too superficial, lacks necessary detail or explanation.\n",
        "   - 3: Provides adequate depth for the request.\n",
        "   - 5: Provides comprehensive and insightful detail, exceeding expectations where appropriate.\n",
        "\n",
        "10. **Helpfulness & Actionability**: How well does the output actually achieve the underlying goal of the prompt? Is it useful, practical, or actionable?\n",
        "    - 1: Not helpful, doesn't achieve the prompt's goal.\n",
        "    - 3: Somewhat helpful, partially achieves the goal.\n",
        "    - 5: Very helpful, effectively achieves the prompt's goal, potentially offering extra value.\n",
        "\n",
        "Based on your evaluation, rate the prompt's effectiveness on a scale of 0 to 100 with 100 being perfect and 0 being completely unusable. Be extremely strict during benchmarking - you should ensure that the prompt is absolutely perfect. \n",
        "\n",
        "**Output Format:**\n",
        "Score: [0-100]\n",
        "Feedback: [Detailed feedback on how to improve the prompt]\n",
        "\n",
        "Only output the score and feedback. No additional information is required.\"\"\"\n",
        "    \n",
        "    def analyze(self, prompt: str, output: str) -> Tuple[int, str]:\n",
        "        \"\"\"Analyze prompt and output, return score (0-100) and feedback\"\"\"\n",
        "        analysis_prompt = self.analysis_prompt_template.format(\n",
        "            prompt=prompt,\n",
        "            output=output\n",
        "        )\n",
        "        \n",
        "        try:\n",
        "            response = self.model.generate_content(analysis_prompt)\n",
        "            response_text = response.text.strip()\n",
        "            \n",
        "            # Parse score and feedback\n",
        "            score, feedback = self._parse_response(response_text)\n",
        "            return score, feedback\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing prompt: {e}\")\n",
        "            return 0, f\"Error during analysis: {str(e)}\"\n",
        "    \n",
        "    def _parse_response(self, response_text: str) -> Tuple[int, str]:\n",
        "        \"\"\"Parse score and feedback from analyzer response\"\"\"\n",
        "        # Try to extract score\n",
        "        score_match = re.search(r'Score:\\s*(\\d+)', response_text, re.IGNORECASE)\n",
        "        if score_match:\n",
        "            score = int(score_match.group(1))\n",
        "        else:\n",
        "            # Try alternative patterns\n",
        "            score_match = re.search(r'(\\d+)\\s*/\\s*100', response_text)\n",
        "            if score_match:\n",
        "                score = int(score_match.group(1))\n",
        "            else:\n",
        "                score = 0\n",
        "        \n",
        "        # Extract feedback\n",
        "        feedback_match = re.search(r'Feedback:\\s*(.+?)(?=\\n\\n|$)', response_text, re.DOTALL | re.IGNORECASE)\n",
        "        if feedback_match:\n",
        "            feedback = feedback_match.group(1).strip()\n",
        "        else:\n",
        "            # Try to get everything after score\n",
        "            parts = re.split(r'Score:\\s*\\d+', response_text, flags=re.IGNORECASE)\n",
        "            if len(parts) > 1:\n",
        "                feedback = parts[1].strip()\n",
        "            else:\n",
        "                feedback = \"No feedback provided.\"\n",
        "        \n",
        "        return score, feedback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PromptOptimizerWorkflow:\n",
        "    \"\"\"Main workflow controller that orchestrates all agents\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.question_generator = QuestionGeneratorAgent()\n",
        "        self.prompt_generator = PromptGeneratorAgent()\n",
        "        self.prompt_tester = PromptTesterAgent()\n",
        "        self.analyzer = PromptAnalyzerAgent()\n",
        "        self.state = WorkflowState()\n",
        "    \n",
        "    def start_workflow(self, user_idea: str) -> List[str]:\n",
        "        \"\"\"Start the workflow by generating clarification questions\"\"\"\n",
        "        self.state.user_idea = user_idea\n",
        "        self.state.clarification_questions = self.question_generator.generate_questions(user_idea)\n",
        "        return self.state.clarification_questions\n",
        "    \n",
        "    def submit_answers(self, answers: Dict[str, str]):\n",
        "        \"\"\"Submit user answers to clarification questions\"\"\"\n",
        "        self.state.user_answers = answers\n",
        "    \n",
        "    def run_optimization_loop(self) -> Dict:\n",
        "        \"\"\"Run the main optimization loop\"\"\"\n",
        "        results = {\n",
        "            'final_prompt': '',\n",
        "            'final_score': 0,\n",
        "            'iterations': 0,\n",
        "            'converged': False,\n",
        "            'history': []\n",
        "        }\n",
        "        \n",
        "        # Generate initial prompt\n",
        "        feedback = \"\"\n",
        "        \n",
        "        for iteration in range(MAX_ITERATIONS):\n",
        "            self.state.iteration = iteration + 1\n",
        "            \n",
        "            # Generate prompt\n",
        "            print(f\"\\n--- Iteration {iteration + 1}/{MAX_ITERATIONS} ---\")\n",
        "            print(\"Generating prompt...\")\n",
        "            \n",
        "            self.state.current_prompt = self.prompt_generator.generate_prompt(\n",
        "                self.state.user_idea,\n",
        "                self.state.user_answers,\n",
        "                feedback\n",
        "            )\n",
        "            \n",
        "            if not self.state.current_prompt:\n",
        "                print(\"Error: Failed to generate prompt\")\n",
        "                break\n",
        "            \n",
        "            print(f\"Prompt generated ({len(self.state.current_prompt)} chars)\")\n",
        "            \n",
        "            # Test prompt\n",
        "            print(\"Testing prompt...\")\n",
        "            self.state.prompt_output = self.prompt_tester.test_prompt(self.state.current_prompt)\n",
        "            print(f\"Output generated ({len(self.state.prompt_output)} chars)\")\n",
        "            \n",
        "            # Analyze prompt and output\n",
        "            print(\"Analyzing prompt and output...\")\n",
        "            score, feedback = self.analyzer.analyze(\n",
        "                self.state.current_prompt,\n",
        "                self.state.prompt_output\n",
        "            )\n",
        "            \n",
        "            self.state.current_score = score\n",
        "            self.state.feedback = feedback\n",
        "            self.state.update_best()\n",
        "            \n",
        "            print(f\"Score: {score}/100\")\n",
        "            print(f\"Feedback: {feedback[:200]}...\")\n",
        "            \n",
        "            # Store iteration history\n",
        "            results['history'].append({\n",
        "                'iteration': iteration + 1,\n",
        "                'score': score,\n",
        "                'prompt': self.state.current_prompt,\n",
        "                'output': self.state.prompt_output,\n",
        "                'feedback': feedback\n",
        "            })\n",
        "            \n",
        "            # Check if we've reached target score\n",
        "            if score >= TARGET_SCORE:\n",
        "                print(f\"\\n✓ Target score achieved! ({score}/100)\")\n",
        "                results['final_prompt'] = self.state.current_prompt\n",
        "                results['final_score'] = score\n",
        "                results['iterations'] = iteration + 1\n",
        "                results['converged'] = True\n",
        "                break\n",
        "            \n",
        "            # If this is the last iteration, use best prompt\n",
        "            if iteration == MAX_ITERATIONS - 1:\n",
        "                print(f\"\\n⚠ Maximum iterations reached. Using best prompt (score: {self.state.best_score}/100)\")\n",
        "                results['final_prompt'] = self.state.best_prompt\n",
        "                results['final_score'] = self.state.best_score\n",
        "                results['iterations'] = MAX_ITERATIONS\n",
        "                results['converged'] = False\n",
        "        \n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage - This will be adapted for Streamlit\n",
        "\n",
        "def run_example():\n",
        "    \"\"\"Example workflow execution\"\"\"\n",
        "    \n",
        "    # Initialize workflow\n",
        "    workflow = PromptOptimizerWorkflow()\n",
        "    \n",
        "    # Step 1: User enters idea\n",
        "    user_idea = input(\"Enter your prompt idea: \")\n",
        "    \n",
        "    # Step 2: Generate questions\n",
        "    print(\"\\nGenerating clarification questions...\")\n",
        "    questions = workflow.start_workflow(user_idea)\n",
        "    \n",
        "    print(\"\\nPlease answer the following questions:\\n\")\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"{i}. {question}\")\n",
        "    \n",
        "    # Step 3: Collect answers\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    answers = {}\n",
        "    for i, question in enumerate(questions, 1):\n",
        "        answer = input(f\"\\nAnswer to question {i}: \")\n",
        "        answers[question] = answer\n",
        "    \n",
        "    workflow.submit_answers(answers)\n",
        "    \n",
        "    # Step 4: Run optimization loop\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Starting optimization loop...\")\n",
        "    print(\"=\"*50)\n",
        "    \n",
        "    results = workflow.run_optimization_loop()\n",
        "    \n",
        "    # Step 5: Display results\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL RESULTS\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Final Score: {results['final_score']}/100\")\n",
        "    print(f\"Iterations: {results['iterations']}\")\n",
        "    print(f\"Converged: {results['converged']}\")\n",
        "    print(\"\\nFinal Prompt:\")\n",
        "    print(\"-\"*50)\n",
        "    print(results['final_prompt'])\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Uncomment to run example:\n",
        "# results = run_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Streamlit-Ready Functions\n",
        "\n",
        "The following functions are designed to be easily integrated into a Streamlit app:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n# streamlit_app.py structure:\\n#\\n# import streamlit as st\\n# import google.generativeai as genai\\n# from prompt_optimizer_workflow import (\\n#     initialize_workflow, \\n#     get_clarification_questions, \\n#     process_answers_and_optimize\\n# )\\n#\\n# # Configure API key from Streamlit secrets\\n# # Make sure you have .streamlit/secrets.toml with:\\n# # GOOGLE_API_KEY = \"your-api-key-here\"\\n# if \\'GOOGLE_API_KEY\\' in st.secrets:\\n#     genai.configure(api_key=st.secrets[\\'GOOGLE_API_KEY\\'])\\n# else:\\n#     st.error(\"Please configure GOOGLE_API_KEY in Streamlit secrets\")\\n#     st.stop()\\n#\\n# if \\'workflow\\' not in st.session_state:\\n#     st.session_state.workflow = initialize_workflow()\\n#     st.session_state.stage = \\'idea\\'\\n#\\n# if st.session_state.stage == \\'idea\\':\\n#     st.title(\"Prompt Optimizer\")\\n#     user_idea = st.text_input(\\'Enter your prompt idea:\\')\\n#     if user_idea:\\n#         with st.spinner(\\'Generating clarification questions...\\'):\\n#             questions = get_clarification_questions(st.session_state.workflow, user_idea)\\n#         st.session_state.questions = questions\\n#         st.session_state.stage = \\'questions\\'\\n#\\n# elif st.session_state.stage == \\'questions\\':\\n#     st.title(\"Answer Clarification Questions\")\\n#     answers = {}\\n#     for q in st.session_state.questions:\\n#         answers[q] = st.text_input(f\\'Q: {q}\\', key=q)\\n#     if st.button(\\'Submit Answers\\'):\\n#         with st.spinner(\\'Optimizing prompt... This may take a few minutes.\\'):\\n#             results = process_answers_and_optimize(st.session_state.workflow, answers)\\n#         st.session_state.results = results\\n#         st.session_state.stage = \\'results\\'\\n#\\n# elif st.session_state.stage == \\'results\\':\\n#     st.title(\"Final Optimized Prompt\")\\n#     st.write(f\"**Score:** {st.session_state.results[\\'final_score\\']}/100\")\\n#     st.write(f\"**Iterations:** {st.session_state.results[\\'iterations\\']}\")\\n#     st.write(f\"**Converged:** {\\'Yes\\' if st.session_state.results[\\'converged\\'] else \\'No (max iterations reached)\\'}\")\\n#     st.write(\"**Final Prompt:**\")\\n#     st.code(st.session_state.results[\\'final_prompt\\'], language=\\'xml\\')\\n#     \\n#     if st.button(\\'Start Over\\'):\\n#         for key in list(st.session_state.keys()):\\n#             del st.session_state[key]\\n#         st.rerun()\\n#\\n# # Note: Create .streamlit/secrets.toml file with:\\n# # GOOGLE_API_KEY = \"your-actual-api-key\"\\n'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def initialize_workflow() -> PromptOptimizerWorkflow:\n",
        "    \"\"\"Initialize the workflow - call this at the start of Streamlit session\"\"\"\n",
        "    return PromptOptimizerWorkflow()\n",
        "\n",
        "def get_clarification_questions(workflow: PromptOptimizerWorkflow, user_idea: str) -> List[str]:\n",
        "    \"\"\"Get clarification questions for user idea\"\"\"\n",
        "    return workflow.start_workflow(user_idea)\n",
        "\n",
        "def process_answers_and_optimize(workflow: PromptOptimizerWorkflow, answers: Dict[str, str]) -> Dict:\n",
        "    \"\"\"Process user answers and run optimization loop\"\"\"\n",
        "    workflow.submit_answers(answers)\n",
        "    return workflow.run_optimization_loop()\n",
        "\n",
        "# Example Streamlit integration structure:\n",
        "\"\"\"\n",
        "# streamlit_app.py structure:\n",
        "#\n",
        "# import streamlit as st\n",
        "# import google.generativeai as genai\n",
        "# from prompt_optimizer_workflow import (\n",
        "#     initialize_workflow, \n",
        "#     get_clarification_questions, \n",
        "#     process_answers_and_optimize\n",
        "# )\n",
        "#\n",
        "# # Configure API key from Streamlit secrets\n",
        "# # Make sure you have .streamlit/secrets.toml with:\n",
        "# # GOOGLE_API_KEY = \"your-api-key-here\"\n",
        "# if 'GOOGLE_API_KEY' in st.secrets:\n",
        "#     genai.configure(api_key=st.secrets['GOOGLE_API_KEY'])\n",
        "# else:\n",
        "#     st.error(\"Please configure GOOGLE_API_KEY in Streamlit secrets\")\n",
        "#     st.stop()\n",
        "#\n",
        "# if 'workflow' not in st.session_state:\n",
        "#     st.session_state.workflow = initialize_workflow()\n",
        "#     st.session_state.stage = 'idea'\n",
        "#\n",
        "# if st.session_state.stage == 'idea':\n",
        "#     st.title(\"Prompt Optimizer\")\n",
        "#     user_idea = st.text_input('Enter your prompt idea:')\n",
        "#     if user_idea:\n",
        "#         with st.spinner('Generating clarification questions...'):\n",
        "#             questions = get_clarification_questions(st.session_state.workflow, user_idea)\n",
        "#         st.session_state.questions = questions\n",
        "#         st.session_state.stage = 'questions'\n",
        "#\n",
        "# elif st.session_state.stage == 'questions':\n",
        "#     st.title(\"Answer Clarification Questions\")\n",
        "#     answers = {}\n",
        "#     for q in st.session_state.questions:\n",
        "#         answers[q] = st.text_input(f'Q: {q}', key=q)\n",
        "#     if st.button('Submit Answers'):\n",
        "#         with st.spinner('Optimizing prompt... This may take a few minutes.'):\n",
        "#             results = process_answers_and_optimize(st.session_state.workflow, answers)\n",
        "#         st.session_state.results = results\n",
        "#         st.session_state.stage = 'results'\n",
        "#\n",
        "# elif st.session_state.stage == 'results':\n",
        "#     st.title(\"Final Optimized Prompt\")\n",
        "#     st.write(f\"**Score:** {st.session_state.results['final_score']}/100\")\n",
        "#     st.write(f\"**Iterations:** {st.session_state.results['iterations']}\")\n",
        "#     st.write(f\"**Converged:** {'Yes' if st.session_state.results['converged'] else 'No (max iterations reached)'}\")\n",
        "#     st.write(\"**Final Prompt:**\")\n",
        "#     st.code(st.session_state.results['final_prompt'], language='xml')\n",
        "#     \n",
        "#     if st.button('Start Over'):\n",
        "#         for key in list(st.session_state.keys()):\n",
        "#             del st.session_state[key]\n",
        "#         st.rerun()\n",
        "#\n",
        "# # Note: Create .streamlit/secrets.toml file with:\n",
        "# # GOOGLE_API_KEY = \"your-actual-api-key\"\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Testing and Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test individual components\n",
        "\n",
        "def test_question_generator():\n",
        "    \"\"\"Test the question generator\"\"\"\n",
        "    agent = QuestionGeneratorAgent()\n",
        "    questions = agent.generate_questions(\"I want to create a prompt for writing blog posts\")\n",
        "    print(\"Generated Questions:\")\n",
        "    for i, q in enumerate(questions, 1):\n",
        "        print(f\"{i}. {q}\")\n",
        "    return questions\n",
        "\n",
        "def test_prompt_generator():\n",
        "    \"\"\"Test the prompt generator\"\"\"\n",
        "    agent = PromptGeneratorAgent()\n",
        "    user_idea = \"I want to create a prompt for writing blog posts\"\n",
        "    answers = {\n",
        "        \"What is the main purpose?\": \"To write engaging blog posts about technology\",\n",
        "        \"Who is the target audience?\": \"Tech enthusiasts and developers\",\n",
        "        \"What tone should it have?\": \"Professional but friendly\"\n",
        "    }\n",
        "    prompt = agent.generate_prompt(user_idea, answers)\n",
        "    print(\"Generated Prompt:\")\n",
        "    print(prompt)\n",
        "    return prompt\n",
        "\n",
        "# Uncomment to test:\n",
        "# test_question_generator()\n",
        "# test_prompt_generator()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
